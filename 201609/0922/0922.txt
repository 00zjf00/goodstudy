hadoop 的 MapReduce

MapReduce执行过程
MapReduce运行时，会通过Mapper运行的任务读取HDFS中的数据文件，然后调用自己的方法，处理数据最后输出。Reducer任务会接受Mapper任务输出的数据，作为自己的输入，调用自己的方法，最后输出到HDFS的文件中。

流程见1.png

Mapper任务的执行过程
每一个Mapper任务是一个java过程，它或读取HDFS中的文件，解析成很多的键值对，经过我们覆盖的map方法处理后，转换为很多的键值对在输出。

流程见 2.png

1、将数据分片==>输入片
2、将输入片解析成键值对
3、对每一个键值对调用map方法，输出零个或多个键值对
4、对输出的键值对进行分区
5、对每个分区的键值对进行排序
*6、对数据进行规约处理，也就是reduce处理（默认没有）

Reducer任务的执行过程
每个Reducer任务是一个java进程。Reducer任务接受Mapper任务的输出，归约处理后写入到HDFS中。

流程见 3.png

1、从Mapper任务复制其输出的键值对输出到本地
2、对本地数据进行合并，排序
3、对排序后的键值对调用reduce方法。键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对。最后把这些输出的键值对写入到HDFS文件中

HDFS:Hadoop分布式文件系统

HDFS有很多特点：

    ① 保存多个副本，且提供容错机制，副本丢失或宕机自动恢复。默认存3份。

    ② 运行在廉价的机器上。

    ③ 适合大数据的处理。多大？多小？HDFS默认会将文件分割成block，64M为1个block。然后将block按键值对存储在HDFS上，并将键值对的映射存到内存中。如果小文件太多，那内存的负担会很重。
