caffe 反向传播
反向传播过程只有在训练过程下才需要计算，消耗时间较长，对计算资源要求较高，一般为离线服务。
损失函数是在现象传播计算中得到的，同时也是反向传播的起点。
一个非常差的分类器，可能在真实标签上的分类概率接近于0，那么损失函数就接近于正无穷，我们成为训练发散，需要调小学习速率。
6.9高原反应：训练了好久loss都不下降，说明还没有训练收敛的迹象，尝试跳大学习速率，或者修改权值初始化方式。
层函数： LayerSetUp：层配置函数   Reshape：变形函数 
caffe 最优化求解过程
	 求解器的特性如下：
	 1、负责记录优化过程，创建用于学习的训练网络和用于评估学习效果的测试网络
	 2、调用Forward --> 调用 Backward--> 更新权值，反复迭代优化模型
	 3、周期性地评估测试网络
	 4、在优化过程中为模型、求解器状态打快照
	 求解器在每次迭代中做了如下事情：
	 1、调用Net的前向传播函数来计算输入和损失函数
	 2、调用Net的反向传播函数来计算梯度
	 3、根据求解器方法，将梯度转换为权值增量
	 4、根据学习速率、历史权值、所用方法更新求解器状态

	 如果训练发散（损失函数值变得非常大，甚至出现NaN或Inf），试着减小base_lr（学习速率），然后重新训练，直到不再发散

	 求解器方法重点是最小化损失函数的全局优化问题

	 学习速率衰减策略：
	 	fixed： 固定学习速率，始终等于base_lr
	 	step： 步进衰减，等于base_lr*gamma^(floor(iter/step))
	 	exp： 指数衰减，等于base_lr*gamma^iter
	 	inv: 倒数衰减，等于base_lr*(1+gamma*iter)^(-power)
	 	multistep: 多步衰减，与步进衰减类似，允许非均匀步进值(stepvalue)
	 	poly： 多项式衰减，在max_iter时达到0。计算公式为base_lr（1-iter/max_iter）^(power)
	 	sigmoid: sigmoid衰减，等于base_lr（1/（1+exp（-gamma*（iter-stepsize））））

